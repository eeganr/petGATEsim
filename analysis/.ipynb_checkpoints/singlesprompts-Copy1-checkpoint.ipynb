{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6606902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pip3` not found.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scipy uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60f535ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import root_scalar\n",
    "import struct\n",
    "# import uproot\n",
    "\n",
    "# === CONFIG ===\n",
    "INFILE = \"/scratch/users/eeganr/pastoutput/output15Singles.dat\"\n",
    "NUM_VOLIDS = 6\n",
    "cont_magnitude = 1e-5\n",
    "num_TOF_bins = 9\n",
    "TOF_bin_width = 29\n",
    "sigma_TOF = 60\n",
    "num_iterations = 2\n",
    "num_subsets = 8\n",
    "\n",
    "total_time = 60.0 # total sim time (s)\n",
    "TAU = 1.2e-8 # coincidence window (s)\n",
    "DELAY = TAU # delayed window offset (s)\n",
    "num_detectors = 48 * 48\n",
    "\n",
    "image_shape = (200, 200, 700)  # (x, y, z) voxels # originally (310, 310, 310)\n",
    "voxel_size = (0.1, 0.1, 0.1)  #mm #originally (1,1,1)\n",
    "radius_mm = 10 # orinally 130 (mm)\n",
    "use_tof = True # originally False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5f91b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === User Configuration ===\n",
    "FILE_TYPE  = \"singles\"                  # Choose: \"hits\", \"singles\" or \"coinc\"\n",
    "NUM_VOLIDS = 6                         # Number of volume ID levels (e.g. 6 for cylindricalPET)\n",
    "# USER_MASK: None = include all specs; otherwise provide a 0/1 list\n",
    "#   length = 20 for hits, 18 for singles, 36 for coincidences\n",
    "#   mask = [1]*36\n",
    "USER_MASK  = None\n",
    "OUTCSV     = \"output.csv\"           # Path to output CSV\n",
    "\n",
    "import struct\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Build specs for gateHits.dat (.dat)\n",
    "def build_hits_specs(num_vol_ids):\n",
    "    return [\n",
    "        (\"run\",             \"i\", 1),\n",
    "        (\"event\",           \"i\", 1),\n",
    "        (\"primaryID\",       \"i\", 1),\n",
    "        (\"sourceID\",        \"i\", 1),\n",
    "        (\"volIDs\",          \"i\", num_vol_ids),\n",
    "        (\"time\",            \"d\", 1),\n",
    "        (\"Edep\",            \"d\", 1),\n",
    "        (\"range\",           \"d\", 1),\n",
    "        (\"posX\",            \"d\", 1),\n",
    "        (\"posY\",            \"d\", 1),\n",
    "        (\"posZ\",            \"d\", 1),\n",
    "        (\"geant4_code\",     \"i\", 1),\n",
    "        (\"particleID\",      \"i\", 1),\n",
    "        (\"motherID\",        \"i\", 1),\n",
    "        (\"photonID\",        \"i\", 1),\n",
    "        (\"nComPhantom\",     \"i\", 1),\n",
    "        (\"nRayPhantom\",     \"i\", 1),\n",
    "        (\"process\",         \"8s\",1),\n",
    "        (\"lastComptonVol\",  \"8s\",1),\n",
    "        (\"lastRayleighVol\", \"8s\",1),\n",
    "    ]\n",
    "\n",
    "# Build specs for gateSingles.dat\n",
    "def build_singles_specs(num_vol_ids):\n",
    "    return [\n",
    "        (\"run\",       \"i\", 1),\n",
    "        (\"event\",     \"i\", 1),\n",
    "        (\"srcID\",     \"i\", 1),\n",
    "        (\"srcX\",      \"d\", 1),\n",
    "        (\"srcY\",      \"d\", 1),\n",
    "        (\"srcZ\",      \"d\", 1),\n",
    "        (\"volIDs\",    \"i\", num_vol_ids),\n",
    "        (\"time\",      \"d\", 1),\n",
    "        (\"Edep\",      \"d\", 1),\n",
    "        (\"detX\",      \"d\", 1),\n",
    "        (\"detY\",      \"d\", 1),\n",
    "        (\"detZ\",      \"d\", 1),\n",
    "        (\"nComPh\",    \"i\", 1),\n",
    "        (\"nComDet\",   \"i\", 1),\n",
    "        (\"nRayPh\",    \"i\", 1),\n",
    "        (\"nRayDet\",   \"i\", 1),\n",
    "        (\"phantomCom\",\"8s\",1),\n",
    "        (\"phantomRay\",\"8s\",1),\n",
    "    ]\n",
    "\n",
    "# Build specs for gateCoincidences.dat by combining two singles\n",
    "def build_coinc_specs(num_vol_ids):\n",
    "    single = build_singles_specs(num_vol_ids)\n",
    "    specs = []\n",
    "    for i in (1, 2):\n",
    "        for name, fmt, cnt in single:\n",
    "            specs.append((f\"{name}{i}\", fmt, cnt))\n",
    "    return specs\n",
    "\n",
    "# Convert specs + mask into struct and field names\n",
    "def build_struct_and_fields(specs, mask):\n",
    "    if len(mask) != len(specs):\n",
    "        raise ValueError(f\"Mask length {len(mask)} != number of specs {len(specs)}\")\n",
    "    fmt = \"<\"\n",
    "    fields = []\n",
    "    for keep, (name, fchar, cnt) in zip(mask, specs):\n",
    "        if keep:\n",
    "            fmt += f\"{cnt}{fchar}\"\n",
    "            if cnt == 1:\n",
    "                fields.append(name)\n",
    "            else:\n",
    "                for idx in range(cnt):\n",
    "                    fields.append(f\"{name}_{idx}\")\n",
    "    return struct.Struct(fmt), fields\n",
    "\n",
    "# Read .dat file and yield each record as a dict\n",
    "def parse_file(path, st, fields):\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(st.size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            yield dict(zip(fields, st.unpack(chunk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "240462d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "unpack requires a buffer of 152 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13101/3142499582.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Parse and collect records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13101/3153958214.py\u001b[0m in \u001b[0;36mparse_file\u001b[0;34m(path, st, fields)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31merror\u001b[0m: unpack requires a buffer of 152 bytes"
     ]
    }
   ],
   "source": [
    "if FILE_TYPE == \"hits\":\n",
    "        specs = build_hits_specs(NUM_VOLIDS)\n",
    "elif FILE_TYPE == \"singles\":\n",
    "    specs = build_singles_specs(NUM_VOLIDS)\n",
    "else:\n",
    "    specs = build_coinc_specs(NUM_VOLIDS)\n",
    "\n",
    "# Default to all fields if no USER_MASK provided\n",
    "mask = USER_MASK if USER_MASK is not None else [1] * len(specs)\n",
    "st, fields = build_struct_and_fields(specs, mask)\n",
    "print(st.size)\n",
    "\n",
    "# Parse and collect records\n",
    "records = list(parse_file(INFILE, st, fields))\n",
    "df = pd.DataFrame(records, columns=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de330ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30e518d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.getsize(INFILE) & 132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bdae28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3439"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bdd46977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from ROOT files\n",
    "\n",
    "def get_all_vals(file, name):\n",
    "    num = max([int(i.split(';')[1]) for i in file.keys() if i.split(';')[0] == name])\n",
    "    return file[f'{name};{num}']\n",
    "\n",
    "with uproot.open(INFILE) as file:\n",
    "    singles_tree = get_all_vals(file, 'Singles')\n",
    "    coincidence_tree = get_all_vals(file, 'Coincidences')\n",
    "\n",
    "    singles = pd.DataFrame({\n",
    "        \"time\": singles_tree[\"time\"].array(library=\"np\"),\n",
    "        \"detector\": singles_tree[\"crystalID\"].array(library=\"np\"),\n",
    "        \"source\": list(map(tuple, np.stack((singles_tree[\"sourcePosX\"].array(library=\"np\"), \n",
    "                            singles_tree[\"sourcePosY\"].array(library=\"np\"), \n",
    "                            singles_tree[\"sourcePosZ\"].array(library=\"np\")), axis=-1))),\n",
    "    })\n",
    "    coincidences = pd.DataFrame({\n",
    "        \"time1\": coincidence_tree[\"time1\"].array(library=\"np\"),\n",
    "        \"time2\": coincidence_tree[\"time2\"].array(library=\"np\"),\n",
    "        \"detector1\": coincidence_tree[\"crystalID1\"].array(library=\"np\"),\n",
    "        \"detector2\": coincidence_tree[\"crystalID2\"].array(library=\"np\"),\n",
    "        \"source1\": list(map(tuple, np.stack((coincidence_tree[\"sourcePosX1\"].array(library=\"np\"), \n",
    "                            coincidence_tree[\"sourcePosY1\"].array(library=\"np\"), \n",
    "                            coincidence_tree[\"sourcePosZ1\"].array(library=\"np\")), axis=-1))),\n",
    "        \"source2\": list(map(tuple, np.stack((coincidence_tree[\"sourcePosX2\"].array(library=\"np\"), \n",
    "                            coincidence_tree[\"sourcePosY2\"].array(library=\"np\"), \n",
    "                            coincidence_tree[\"sourcePosZ2\"].array(library=\"np\")), axis=-1))),\n",
    "    })\n",
    "    coincidences['true'] = coincidences['source1'] == coincidences['source2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3564a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System-Wide Equation Constants\n",
    "\n",
    "S = len(singles) / total_time # Rate of singles measured by scanner as a whole\n",
    "P = 2 * len(coincidences) / total_time # Twice the prompts rate\n",
    "\n",
    "# Roots of this function are the lambda (L) values.\n",
    "def lambda_eq(L):\n",
    "    return 2 * TAU * L * L - L + S - P * np.exp((L + S)*TAU)\n",
    "\n",
    "L = root_scalar(lambda_eq, x0=0)\n",
    "if not L.converged:\n",
    "    raise RuntimeError(\"Failed to converge on lambda.\")\n",
    "L = L.root\n",
    "\n",
    "det1_counts = coincidences['detector1'].value_counts().to_dict() # det1 coincidences involved\n",
    "det2_counts = coincidences['detector2'].value_counts().to_dict() # det2 coincidences involved\n",
    "\n",
    "prompts = pd.DataFrame({'detector': list(range(num_detectors))})\n",
    "prompts['prompts'] = prompts['detector'].map(lambda x: det1_counts.get(x, 0) + det2_counts.get(x, 0))\n",
    "prompt_count = prompts.set_index('detector')['prompts'].to_dict()\n",
    "singles_counts = singles['detector'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "85375cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the randoms rate from a pair of detectors with crystalIDs i and j\n",
    "def randomsrate(i, j):\n",
    "    P_i = prompt_count.get(i, 0) / total_time\n",
    "    P_j = prompt_count.get(j, 0) / total_time\n",
    "    S_i = singles_counts.get(i, 0) / total_time\n",
    "    S_j = singles_counts.get(j, 0) / total_time\n",
    "    coeff = (2 * TAU * np.exp(-(L + S)*TAU))/((1 - 2 * L * TAU)**2)\n",
    "    i_term = S_i - np.exp((L + S)*TAU) * P_i\n",
    "    j_term = S_j - np.exp((L + S)*TAU) * P_j\n",
    "    return coeff * i_term * j_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d3412fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304\n",
      "Processing detector 200/2304, total = 150.49356489781647\n",
      "Processing detector 400/2304, total = 150.49356489781647\n",
      "Processing detector 600/2304, total = 150.49356489781647\n",
      "Processing detector 800/2304, total = 150.49356489781647\n",
      "Processing detector 1000/2304, total = 150.49356489781647\n",
      "Processing detector 1200/2304, total = 150.49356489781647\n",
      "Processing detector 1400/2304, total = 150.49356489781647\n",
      "Processing detector 1600/2304, total = 150.49356489781647\n",
      "Processing detector 1800/2304, total = 150.49356489781647\n",
      "Processing detector 2000/2304, total = 150.49356489781647\n",
      "Processing detector 2200/2304, total = 150.49356489781647\n",
      "2.457448766556835\n"
     ]
    }
   ],
   "source": [
    "sp_estimate = 0\n",
    "detectors = singles['detector'].unique()\n",
    "for i in range(num_detectors):\n",
    "    if (i + 1) % 200 == 0:\n",
    "        print(f\"Processing detector {i + 1}/{len(detectors)}, total = \" + str(total))\n",
    "    for j in range(i, num_detectors):\n",
    "        sp_estimate += randomsrate(i, j)\n",
    "\n",
    "print(sp_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8259ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_estimate = sp_estimate * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "46b0b41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147.4469259934101\n"
     ]
    }
   ],
   "source": [
    "print(sp_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cd3af3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = coincidences[coincidences['true'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ea7cacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a107c955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n"
     ]
    }
   ],
   "source": [
    "dw_estimate = 0\n",
    "for t in singles['time']:\n",
    "    dw_estimate += np.searchsorted(singles['time'], t + DELAY + TAU) - np.searchsorted(singles['time'], t + TAU)\n",
    "print(dw_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8b30c75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing detector 200/2304, total = 0.4858576652999432\n",
      "Processing detector 400/2304, total = 0.9384399817929115\n",
      "Processing detector 600/2304, total = 1.3583664178806412\n",
      "Processing detector 800/2304, total = 1.733067151955013\n",
      "Processing detector 1000/2304, total = 2.0639006815556997\n",
      "Processing detector 1200/2304, total = 2.346096633095667\n",
      "Processing detector 1400/2304, total = 2.581322612982198\n",
      "Processing detector 1600/2304, total = 2.7671906837088227\n",
      "Processing detector 1800/2304, total = 2.9058936105887465\n",
      "Processing detector 2000/2304, total = 2.996275632968659\n",
      "Processing detector 2200/2304, total = 3.040703559228618\n"
     ]
    }
   ],
   "source": [
    "sr_estimate = 0\n",
    "for i in range(num_detectors):\n",
    "    if (i + 1) % 200 == 0:\n",
    "        print(f\"Processing detector {i + 1}/{len(detectors)}, total = \" + str(sr_estimate))\n",
    "    for j in range(i, num_detectors):\n",
    "        sr_estimate += 2 * TAU * singles_counts.get(i, 0) / total_time * singles_counts.get(j, 0) / total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6ebdb762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182.79833424051856\n"
     ]
    }
   ],
   "source": [
    "print(sr_estimate * total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73226e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
